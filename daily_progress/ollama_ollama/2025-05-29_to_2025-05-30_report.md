# ollama 项目进展

## 时间周期：2025-05-29至2025-05-30

## 新增功能
- 允许`ollama pull`接受多个参数，以提高灵活性。
- 在ggml中添加了导出GPU UUIDs的功能。

## 主要改进
- 进行了一些与多GPU支持相关的讨论，如Qwen3-32B-128K-GGUF在多GPU环境下无法使用。
- 优化了模型的推理速度相关的提案，探讨将剩余的GPU资源用于提升模型性能。
- 标题为“增强全工具调用生命周期（包括后续响应）的流式支持”的特性请求被提出。

## 修复问题
- 修复了多个关于不支持架构的转换错误，如从safetensors转换时的DeepseekV3ForCausalLM和Qwen3ForCausalLM的架构问题。
- 解决了Ollama在MacOS上报告版本不匹配的问题。
- 修复Ollama Docker版本高于0.6.5时无法运行的问题。
- 处理了Ollama从内存中频繁清除Qwen3的问题。
- 解决了在720p或更高分辨率图像下Qwen2.5-VL 32b崩溃的问题。
- 针对内存泄露的可能性进行了修复，并优化了错误信息的清晰度（如“POST predict”错误信息）。