# Progress for ollama/ollama (2025-05-29 to 2025-05-30)


## Issues Closed in the Last 1 Days
- Error converting from safetensors => Error: unsupported architecture "DeepseekV3ForCausalLM" #10904
- Cannot run ollama docker any version higher than 0.6.5 #10902
- Ollama Reporting Version Client Mismatch on MacOS #10901
- Allow ollama pull to accept multiple arguments #10890 #7997 #10895
- ggml: Export GPU UUIDs #10892
- Allow `ollama pull` to accept multiple arguments. #10890
- Am I creating a quantized model correctly - resulting model outputs random characters / gibberish? #10885
- **Title: Feature Request: Robust Streaming Support for the Full Tool Calling Lifecycle (including subsequent responses)** #10870
- unsloth/Qwen3-32B-128K-GGUF can not use multi GPU #10832
- when it can support agent2agent framework? #10827
- gpu没有跑满，是否可以将剩余的gpu全部用来提升模型的推理速度 #10795
- Check GPU / CPU without actually running model #10776
- Ollama 4 Maverick download stopped #10768
- Ollama keeps flushing Qwen3 from memory every prompt #10762
- Qwen2.5-VL 32b crashes with 720p or higher resolution image #10753
- RAM is almost always used #10745
- iMac not using GPU #10715
- Possible memory leakage #10709
- llm: Make "POST predict" error message more informative #10702
- Error with llama 3.3 q8 models and variations #10626
- f-lite image generation model #10606
- Error converting from safetensors" error="unsupported architecture "Qwen3ForCausalLM" #10602
- LLAMA4 Scout crashes during run #10532
- Request for Manual GPU Assignment in Ollama: Feature Proposal to Specify Target GPUs for Model Loading #10531
- ollama for amd #10426
- Ollama Pull Command throwing error #10403
- Ollama is not utilizing GPU #10381
- Support to GLM-Z1-32B, GLM-Z1-Rumination-32B #10298
- Model Request: ALLaM-AI/ALLaM-7B-Instruct-preview #10271
- There are ads on the official website models? #10206
