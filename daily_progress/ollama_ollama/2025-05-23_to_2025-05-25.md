# Progress for ollama/ollama (2025-05-23 to 2025-05-25)


## Issues Closed in the Last 2 Days
- Add Gemma 3n support #10854
- ollama upgrade 0.7.1. breaks gemma-3-12b:latest #10853
- Ubuntu 22.04.05 offline environment can't upgrade version #10850
- [Documentation] OLLAMA_MODELS missing info in FAQ for Linux #10847
- server: add hint to the error message when model path access fails #10843
- docs: remove unsupported quantizations #10842
- more control over web search settings; multiple configured search providers, and per-user settings. #10841
- ml: Improve slog formatting for BackendMemory #10840
- /usr/share/ollama isn't usable as a symlink #10839
- tests: drop llama3.2-vision embedding tests #10837
- mistral-small3.1:latest: OLLAMA_CONTEXT_LENGTH=16384 but ollama runs with --ctx-size 4096 #10829
- Please provide the small parameter model and the distilled model of deepseek-v3. #10826
- v0.7.0 release on Linux actually contains v0.6.2 binary #10821
- Add discover for Intel GPU  and support OneApi SYCL #10816
- devstral:24b-small-2505-q8_0 immediately fails to load. #10808
- readme: Add macLlama to community integrations #10790
- Ollama unable to interpret a local SSL Cert #10783
- How should I run multiple models in one time? #10738
- Ollama 0.7.0 decreased the quality of Llama-vision dramatically. #10733
- degraded response quality with llama3.2-vision + 0.7.0 #10731
- fix crash in old clients with quantization progress #10710
- Reporting compiler WARNING messages during build of ollama main branch using gcc 15.1.0 #10706
- Error: llama runner process has terminated: GGML_ASSERT(n_backends <= GGML_SCHED_MAX_BACKENDS) failed #10705
- ollama not using AMD GPU and using CPU instead #10691
- Ollama run/pull <MODEL_NAME> is not working #10689
- Not working #10686
- I get this quite often what could be reason using it via api on swe agent #10684
- 6K context, only thtough API: error loading llama server "timed out waiting for llama runner to start: context canceled" #10671
- Error while tool calling from agno #10669
- key not found #10668

## Pull Requests Merged in the Last 2 Days
- server: add hint to the error message when model path access fails #10843
- docs: remove unsupported quantizations #10842
- ml: Improve slog formatting for BackendMemory #10840
- tests: drop llama3.2-vision embedding tests #10837
- llama: add minimum memory for grammar #10820
- sched: fix runner leak during reloading unload #10819
- Add discover for Intel GPU  and support OneApi SYCL #10816
- integration: add qwen2.5-vl #10815
- fix: mllama quality #10807
- server: improve tensor quantization fallback logic #10806
- fix cmakelists #10804
- added shakil #10793
- readme: Add macLlama to community integrations #10790
- fix: qwen25vl assign samebatch in multimodal input #10789
- chore: disable debug in binary libraries #10788
- Memory usage reporting #10787
- win: detect background upgrade in progress #10785
- feat: port qwen2 model #10782
- llama: fix incorrect initialization of C.struct_common_sampler_cparams.penalty_present #10779
- ml: add more rope options #10775
- fix llama and mistral3 models #10774
- llm: Use first layer as memory buffer in estimation #10773
- llm: exclude input layers from VRAM layer size estimation #10770
- Add TinyNotepad to README.md #10763
- avoid kv truncation during create #10761
- remove special case for gemma3 special vocab #10743
- fix: omit array parsing #10723
- remove support for multiple ggufs in a single file #10722
- ggml: Seperate tensor load from backend creation #10721
- Fix lingering Q4_0 help reference #10720
