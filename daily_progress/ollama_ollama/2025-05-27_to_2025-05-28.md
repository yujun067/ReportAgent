# Progress for ollama/ollama (2025-05-27 to 2025-05-28)


## Issues Closed in the Last 1 Days
- kvcache: Skip computing causal mask for worst case graph reservation #10882
- client: add request signing to the client #10881
- Please update learning models to incorporate standard security guidance for CODE/Script/Configuration #10880
- Does the Ollama Docker image support Intel GPU acceleration alongside NVIDIA? #10877
- Llama 3 runs on GPU (fast). Llama 2 runs on CPU (slow) #10874
- AttributeError: 'OllamaLLM' object has no attribute '__pydantic_private__' #10868
- Ollama detect my GPU but model not running on it #10848
- Fix CVE-2025-1975 #10769
- qwen2.5vl:72b-q4_K_M file size (71 GB) appears abnormally large #10729
- Ollama 0.6.6 memory leak with different models #10433
- gemma EOF error on image input due to improper memory management #10041
- Increase the environment variable OLLAMA_NUM_THREAD #9784
- template: Improve ollama model chat template #9735
- Error: llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer (4x L40S, 384GB system RAM, Deepseek-R1) #8597
- Model Request : WhiteRabbitNeo #1911
- Allow listening on all local interfaces #703
